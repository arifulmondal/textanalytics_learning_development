---
title: "Step by Step Text Analytics on My Own Learning Data Log"
author: "Ariful Mondal (ariful dot mondal [at] gmail dot com)"
date: "30 November 2017"
output: 
  html_document:
    toc: true
    toc_depth: 5 
    toc_float: true
    number_sections: true
    code_folding: hide
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
This is a quick text analysis using 3 months of my learning completion data on Lynda and LinkedIn Learning using *R*. Some of the required packages are listed below. 

```{r, warning=FALSE, message=FALSE}
setwd("D:\\RProgramming")
## Load Packages
library("tm")
library("stringi")
library("wordcloud")
library("clue")
library("ggplot2")
library("RColorBrewer")
library("SnowballC")
library("RWeka")

```

- [**tm:**](https://cran.r-project.org/web/packages/tm/tm.pdf) for text mining and natural language processing(NLP)
- [**wordcloud:**](https://cran.r-project.org/web/packages/wordcloud/wordcloud.pdf) to create word clouds
- [**stringi:**](https://cran.r-project.org/web/packages/stringi/stringi.pdf)  to leverage character string processing facilities
- [**clue:**](https://cran.r-project.org/web/packages/clue/clue.pdf) for Cluster Ensembles
- [**SnowballC:**](https://cran.r-project.org/web/packages/SnowballC/SnowballC.pdf) for stemming text
- [**RColorBrewer:**](https://cran.r-project.org/web/packages/RColorBrewer/RColorBrewer.pdf)  color palettes for nice colours of the graphs
- [**RWeka:**](https://cran.r-project.org/web/packages/RWeka/RWeka.pdf) text mining and N-Gram analysis
- [**ggplot2**](https://cran.r-project.org/package=ggplot2/ggplot2.pdf) to Create Elegant Data Visualizations Using the Grammar of Graphics


# Read data and print few lines

You may downlaod this data from my google drive [here](https://drive.google.com/open?id=1KudlYfcJ7KbGvrhcssA4pRpIA3tMswHOw_mwxDTs9xQ)

```{r}
lnd <- readLines("file:///D:/RProgramming/Ariful_Islam_Mondal_Training_Log_Nov_2017.csv",  encoding = "UCS-2LE", skipNul = TRUE)
lnd[1:10]
```

#  Basic text processing and cleaning

- Remove non-English characters, letters etc. using `iconv()` and option `latin1`
- Remove special characters with spaces using `gsub()` and regular expression `[^0-9a-z]`
- Remove duplicate characters using `gsub()` and regular expression
- Remove special numbers with spaces using `gsub()` and regular expression
- Remove multiple spaces to one using `gsub()` and regular expression

To know more on `incon()` click [here](https://www.rdocumentation.org/packages/base/versions/3.4.1/topics/iconv), for `gsub()` click [here](https://www.r-bloggers.com/regular-expression-and-associated-functions-in-r/) and click [here](https://en.wikipedia.org/wiki/Regular_expression) to know more on `regular expression`, also view [`regex`](https://stat.ethz.ch/R-manual/R-devel/library/base/html/regex.html).

```{r}
# Remove non-English characters, letters etc.
# Help ?inconv
lnd<-iconv(lnd, "latin1", "ASCII", sub="")
# Remove special characters with spaces
# Help ?gsub
lnd <- gsub("[^0-9a-z]", " ", lnd, ignore.case = TRUE)
# Remove duplicate characters
lnd <- gsub('([[:alpha:]])\\1+', '\\1\\1', lnd)
# Remove special numbers with spaces
lnd <- gsub("[^a-z]", " ", lnd, ignore.case = TRUE)
# Remove multiple spaces to one
lnd <- gsub("\\s+", " ", lnd)
lnd <- gsub("^\\s", "", lnd)
lnd <- gsub("\\s$", "", lnd)

```

Print after clean up...

```{r}
# Summary
lnd[1:10]
summary(lnd)
str(lnd)
```

# Create a corpus 

Create a virtual corpus using `Vcorpus()` function.

```{r}
# create Corpus
# Help ??VCorpus
myCorpus <- VCorpus(VectorSource(lnd))
myCorpus
```

# Transformation of text 

[Optional for already cleaned data]

Perform necessary transformation/preprocessing activities using `tm_map()` from `tm` package. The objective is to have clean texts by removing stop words, punctuation, multiple white spaces etc. We will perform the following transformations 

- [**tolower:**](https://www.rdocumentation.org/packages/quanteda/versions/0.99.12/topics/toLower) normalize text to small cases. For example `My name Is Ariful` will be converted to small case `my name is ariful`.
- [**stopwords:**](https://www.rdocumentation.org/packages/tm/versions/0.7-1/topics/stopwords) remove stop words using English dictionary. For example remove words like "a", "and", "but", "how", "or", and "what"
- [**removePunctuation:**](https://www.rdocumentation.org/packages/tm/versions/0.7-1/topics/removePunctuation) remove punctuation marks such as ! " # $ % & ' ( ) * + , - . / : ; < = > ? @ [ \ ] ^ _ ` { | } ~.
- [**stripWhitespace:**](https://www.rdocumentation.org/packages/tm/versions/0.7-1/topics/stripWhitespace) Strip extra white space from a text document. Multiple white space characters are collapsed to a single blank
- [**PlainTextDocument:**](https://www.rdocumentation.org/packages/tm/versions/0.7-1/topics/PlainTextDocument) convert document to plain text format

```{r, eval=FALSE}
# Help ??tm_map'

# Normalize to small cases
myCorpus <- tm_map(myCorpus, content_transformer(tolower))  

# Remove Stop Words
myCorpus <- tm_map(myCorpus, removeWords, stopwords("english"))

# Remove Punctuation
myCorpus <- tm_map(myCorpus, removePunctuation)

# Remove Numbers 
 myCorpus <- tm_map(myCorpus, removeNumbers)

# Create plain text documents
myCorpus <- tm_map(myCorpus, PlainTextDocument)

# Stem words in a text document using Porter's stemming algorithm.
myCorpus <- tm_map(myCorpus, stemDocument, "english")

# Strip White Spaces
myCorpus <- tm_map(myCorpus, stripWhitespace)

```

# Term Document Matrix & N-Gram Analysis

Now we will use `TermDocumentMatrix()` to create a `document-term matrix` or `term-document matrix` which is a mathematical matrix that describes the frequency of terms/words/strings that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms. There are various schemes for determining the value that each entry in the matrix should take. Read more on [wiki](https://en.wikipedia.org/wiki/Document-term_matrix).


In the fields of `computational linguistics` and `probability`, an `n-gram` is a contiguous sequence of n items from a given sequence of text or speech. The items can be `phonemes`, `syllables`, `letters`, `words` or `base pairs` according to the application. 

The n-grams typically are collected from a text or speech corpus. When the items are words, `n-grams` may also be called shingles.

- An n-gram of size 1 is referred to as a `"unigram"`; 
- An n-gram of size 2 is a `"bigram"` (or, less commonly, a "digram"); 
- An n-gram of size 3 is a `"trigram"`. 

Larger sizes are sometimes referred to by the value of n in modern language, e.g., "*four-gram*", "*five-gram*", and so on. [[wiki](https://en.wikipedia.org/wiki/N-gram)].


## Crate Unigram

```{r}
unitdm <- TermDocumentMatrix(myCorpus)
mat <- as.matrix(unitdm)
wf <- sort(rowSums(mat),decreasing=TRUE)
df <- data.frame(word = names(wf),freq=wf)
head(df, 10)
```

### Ploting unigram

```{r, warning=FALSE, message=FALSE}
barplot(df[1:20,]$freq, las = 2, names.arg = df[1:20,]$word,
        col =df[1:20,]$freq, main ="",
        ylab = "Frequencies", cex.axis=.8, cex = .8, cex.lab=0.75, cex.main=.75)

ggplot(df[1:20,], aes(x = reorder(df[1:20,]$word, df[1:20,]$freq), y = df[1:20,]$freq)) +
    geom_bar(stat = "identity", fill = "#999900") +
    labs(title = " ") +
    xlab("Unigrams") +
    ylab("Frequency")+
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

### Create Word cloud with unigram

```{r}
set.seed(1234)
wordcloud(words = df$word, freq = df$freq, min.freq = 1,
          max.words=100, random.order=FALSE, rot.per=0.75, las=3,
          colors=brewer.pal(8, "Dark2"), c(5,.5), vfont=c("script","plain"))
```

### Find frequently occured words

```{r}
findFreqTerms(unitdm, lowfreq = 5)
```

### Find Associations between Words

Find associations in  document-term or term-document matrix using function `findAssocs(x, terms, corlimit)` from *tm* package.

  - **x**: A DocumentTermMatrix or a TermDocumentMatrix.
  - **terms**: a character vector holding terms.
  - **corlimit**: a numeric vector (of the same length as terms; recycled otherwise) for the (inclusive) lower correlation limits of each term in the range from zero to one.

 1. Find associated words with *data*...

```{r}
findAssocs(unitdm, terms = "data", corlimit = 0.35)
```

 2. Find associated words with *machine*...

```{r}
findAssocs(unitdm, terms = "machine", corlimit = 0.35)
```

 3. Find associated words with *management*...

```{r}
findAssocs(unitdm, terms = "management", corlimit = 0.35)
```

 4. Find associated words with *azure*...

```{r}
findAssocs(unitdm, terms = "azure", corlimit = 0.35)
```

 5. Find associated words with *security*...

```{r}
findAssocs(unitdm, terms = "security", corlimit = 0.35)
```

## Crate Bigram

```{r}
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2)) # Create bigram tokenizer using RWeka
bitdm <- TermDocumentMatrix(myCorpus, control = list(tokenize = BigramTokenizer)) # Create bigram
inspect(bitdm[15:30,1:20]) # Inspect few bigrams
```
```{r}
mat_bigram <- as.matrix(bitdm)
wf_bigram <- sort(rowSums(mat_bigram),decreasing=TRUE)
df1 <- data.frame(word = names(wf_bigram),freq=wf_bigram)
head(df1, 10)
```

### Ploting Bigram

```{r, warning=FALSE, message=FALSE}
biplot<-barplot(df1[1:20,]$freq, las = 2, names.arg = df1[1:20,]$word,
        col = df1[1:20,]$freq, main ="", 
        ylab = "Frequencies", cex.axis=.65, cex = .65, cex.lab=0.5, cex.main=.75)

ggplot(df1[1:20,], aes(x = reorder(df1[1:20,]$word, df1[1:20,]$freq), y = df1[1:20,]$freq)) +
    geom_bar(stat = "identity", fill = "#00b3b3") +
    labs(title = " ") +
    xlab("Bigrams") +
    ylab("Frequency")+
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

### Create Word cloud with Bigram

```{r, warning=F}
set.seed(1234)
wordcloud(words = df1$word, freq = df1$freq, min.freq = 3,
          max.words=100, random.order=T, rot.per=0.75, 
          colors=brewer.pal(8, "Dark2"), c(2,.7), vfont=c("script","plain"))
```



## Crate Trigram

```{r}
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3)) # Create trigram tokenizer using RWeka
tritdm <- TermDocumentMatrix(myCorpus, control = list(tokenize = TrigramTokenizer)) # Create trigram
inspect(tritdm[15:30,1:20]) # Inspect few trigrams
```
```{r}
mat_trigram <- as.matrix(tritdm)
wf_trigram <- sort(rowSums(mat_trigram),decreasing=TRUE)
df2 <- data.frame(word = names(wf_trigram),freq=wf_trigram)
head(df2, 10)
```

### Ploting Trigram

```{r, warning=FALSE, message=FALSE}
triplot<-barplot(df2[1:20,]$freq, las = 2, names.arg = df2[1:20,]$word,
        col = df1[1:20,]$freq, main ="", 
        ylab = "Frequencies", cex.axis=.65, cex = .65, cex.lab=0.5, cex.main=.75)

ggplot(df2[1:20,], aes(x = reorder(df2[1:20,]$word, df2[1:20,]$freq), y = df2[1:20,]$freq)) +
    geom_bar(stat = "identity", fill=df2[1:20,]$freq) +
    labs(title = " ") +
    xlab("Trigrams") +
    ylab("Frequency")+
    theme(axis.text.x = element_text(angle = 45, hjust = 1)) 
```

### Create Word cloud with Trigram

```{r, warning=F}
set.seed(1234)
wordcloud(words = df2$word, freq = df2$freq, min.freq = 3,
          max.words=100, random.order=T, rot.per=0.75, 
          colors=brewer.pal(8, "Dark2"), c(1.7,.7), vfont=c("script","plain"))
```

More coming soon....


--------------------------------------------------------------------------------------------------------------------------

# Appendix

## About R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:

### Including summary

```{r cars}
summary(cars)
```

### Including plots

You can also embed plots, for example:

```{r pressure}
plot(pressure)
```

Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.
